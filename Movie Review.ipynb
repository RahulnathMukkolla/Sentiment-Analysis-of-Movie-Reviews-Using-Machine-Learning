{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d468950d-0c3f-419a-a4e8-04cbf470b22f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 20px;\"><b>Project 2</b></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e8061-2651-46f6-bac7-50cfc7adaafd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 20px;\"><b>Date : 20/10/2023</b></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1501092-7618-4098-8b5d-033e65217fe3",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"><b>Rahulnath Mukkolla</b></span><br/>\n",
    "<span style=\"color: blue\"><b>11669483</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec5e21-bcb7-4dc0-b4e1-0355eb5e78ff",
   "metadata": {},
   "source": [
    "<span style=\"color: red\"> Implement logistic regression using the algorithm provided in the book. Train your model with the training dataset once. The apply the model to the test data set. You will implement cross validation\n",
    "later. Calculate F1 score and accuracy and submit your ipynb file.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "414a477f-211a-4c24-be4c-b7625a785b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f96934be-41bd-493f-b3dd-47753384171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4928\\3218872862.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  given_testing_text =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/test_text.txt?raw=true',sep='delimiter',header=None,names=['tweet'])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4928\\3218872862.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  given_training_text =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/train_text.txt?raw=true',sep='delimiter',header=None,names=['tweet'])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4928\\3218872862.py:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  val_text=pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/val_text.txt?raw=true',sep='delimiter',header=None,names=['tweet'])\n"
     ]
    }
   ],
   "source": [
    "# Reading the train, test, validation labels and text from the given github urls\n",
    "# Reading the csv data using pndas library\n",
    "\n",
    "given_testing_label =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/test_labels.txt?raw=true',header=None,names=['label'])\n",
    "given_training_label =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/train_labels.txt?raw=true',header=None,names=['label'])\n",
    "given_testing_text =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/test_text.txt?raw=true',sep='delimiter',header=None,names=['tweet'])\n",
    "given_training_text =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/train_text.txt?raw=true',sep='delimiter',header=None,names=['tweet'])\n",
    "val_label =pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/val_labels.txt?raw=true',header=None,names=['label'])\n",
    "val_text=pd.read_csv('https://github.com/cardiffnlp/tweeteval/blob/main/datasets/sentiment/val_text.txt?raw=true',sep='delimiter',header=None,names=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219fa8a1-0a20-4961-b028-9dd5e8b3701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of suberddits \n",
    "\n",
    "subreddits = ['3DS.tsv', '4chan.tsv', '2007scape.tsv', 'ACTrade.tsv', 'amiugly.tsv',\n",
    "                   'BabyBumps.tsv', 'baseball.tsv', 'canada.tsv', 'CasualConversation.tsv',\n",
    "                   'DarknetMarkets.tsv', 'darksouls.tsv', 'elderscrollsonline.tsv', 'Eve.tsv',\n",
    "                   'Fallout.tsv', 'fantasyfootball.tsv', 'GameDeals.tsv', 'gamegrumps.tsv',\n",
    "                   'halo.tsv', 'Homebrewing.tsv', 'IAmA.tsv', 'india.tsv', 'jailbreak.tsv',\n",
    "                   'Jokes.tsv', 'KerbalSpaceProgram.tsv', 'Keto.tsv', 'leagueoflegends.tsv',\n",
    "                   'Libertarian.tsv', 'magicTCG.tsv', 'MakeupAddiction.tsv', 'Naruto.tsv',\n",
    "                   'nba.tsv', 'oculus.tsv', 'OkCupid.tsv', 'Parenting.tsv', 'pathofexile.tsv',\n",
    "                   'raisedbynarcissists.tsv', 'Random_Acts_Of_Amazon.tsv', 'science.tsv',\n",
    "                   'Seattle.tsv', 'TalesFromRetail.tsv', 'talesfromtechsupport.tsv',\n",
    "                   'ultrahardcore.tsv', 'videos.tsv', 'Warthunder.tsv', 'whowouldwin.tsv',\n",
    "                   'xboxone.tsv', 'yugioh.tsv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e9246bb-2917-4d59-a0bc-1e88b98bc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and loading the each suberddits, frequent words and adjectives files\n",
    "# Separate the positive and nagative lexicons based on score and stored in seperate lists\n",
    "\n",
    "positive=[]\n",
    "negative=[]\n",
    "\n",
    "\n",
    "files =['1850.tsv',\n",
    "    '1860.tsv',\n",
    "     '1870.tsv',\n",
    "     '1880.tsv',\n",
    "     '1890.tsv',\n",
    "     '1900.tsv',\n",
    "     '1910.tsv',\n",
    "     '1920.tsv',\n",
    "     '1930.tsv',\n",
    "     '1940.tsv',\n",
    "     '1950.tsv',\n",
    "     '1960.tsv',\n",
    "     '1970.tsv',\n",
    "     '1980.tsv',\n",
    "     '1990.tsv',\n",
    "     '2000.tsv',\n",
    "    ]\n",
    "\n",
    "for sub in subreddits:\n",
    "    subreddit = sub.split('.')[0]\n",
    "    data_frame = pd.read_csv(f'subreddits/{sub}', sep='\\t', header=None,names=['word','score1','score2'])\n",
    "    positive.extend(data_frame.loc[data_frame[\"score1\"] >data_frame[\"score2\"], \"word\"].tolist())\n",
    "    negative.extend(data_frame.loc[data_frame[\"score2\"] >data_frame[\"score1\"], \"word\"].tolist())\n",
    "\n",
    "\n",
    "for sub in files:\n",
    "    subreddit = sub.split('.')[0]\n",
    "    data_frame = pd.read_csv(f'frequency/{sub}', sep='\\t', header=None,names=['word','score1','score2'])\n",
    "    positive.extend(data_frame.loc[data_frame[\"score1\"] >data_frame[\"score2\"], \"word\"].tolist())\n",
    "    negative.extend(data_frame.loc[data_frame[\"score2\"] >data_frame[\"score1\"], \"word\"].tolist())\n",
    "    data_frame = pd.read_csv(f'adjective/{sub}', sep='\\t', header=None,names=['word','score1','score2'])\n",
    "    positive.extend(data_frame.loc[data_frame[\"score1\"] >data_frame[\"score2\"], \"word\"].tolist())\n",
    "    negative.extend(data_frame.loc[data_frame[\"score2\"] >data_frame[\"score1\"], \"word\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc53b45-ce70-4bf1-889b-1879c7240f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def feautures(tweet):\n",
    "    tweetre=re.sub(r'[^\\w]',' ',tweet)\n",
    "    tokens=tweetre.split()\n",
    "    ans=[]\n",
    "    #urls\n",
    "    ans.append(tweet.count('.com'))\n",
    "    #hashtags\n",
    "    ans.append(tweet.count('#'))\n",
    "    ans.append(tweet.count('@'))\n",
    "    ans.append(tweet.count('!'))\n",
    "    cc=0\n",
    "    for i in tokens:\n",
    "        #print(i.lower())\n",
    "        if i.lower() in positive:\n",
    "            cc=cc+1\n",
    "    #print(cc)\n",
    "    if cc!=0:\n",
    "        ans.append(np.log(cc))\n",
    "    else:\n",
    "        ans.append(0)\n",
    "    cc=0\n",
    "    for i in tokens:\n",
    "        if i.lower() in negative:\n",
    "            cc=cc+1\n",
    "    #print(cc)\n",
    "    if cc!=0:\n",
    "        ans.append(np.log(cc))\n",
    "    else:\n",
    "        ans.append(0)\n",
    "    emojis=[':)',':D',';)',':p','<3',':P']\n",
    "    cc=0\n",
    "    for i in emojis:\n",
    "        if i in tweet:\n",
    "            cc=cc+1\n",
    "    ans.append(cc)\n",
    "    ans.append(np.log(len(tokens)))\n",
    "    maxi=0\n",
    "    for i in tokens:\n",
    "        maxi=max(maxi,len(i))\n",
    "    ans.append(maxi)\n",
    "    cc=0\n",
    "    for i in tokens:\n",
    "        if len(i)>5:\n",
    "            cc=cc+1\n",
    "    ans.append(cc)\n",
    "    cc=0\n",
    "    for i in tokens:\n",
    "        if i.isupper():\n",
    "            cc=cc+1\n",
    "    ans.append(cc)\n",
    "    cc=0\n",
    "    #count stop words\n",
    "    for i in tokens:\n",
    "        if i in stop_words:\n",
    "            cc=cc+1\n",
    "    ans.append(cc)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82b970dd-5c83-47fd-b8cf-4298aa1aeb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.833213344056216,\n",
       " 3.044522437723423,\n",
       " 0,\n",
       " 3.1780538303479458,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feautures(train_text.iloc[2]['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af98300f-25d3-4276-88cd-f72a34d0234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing only 1000 records on each test\n",
    "# Note: Taking large time to load the data so testing with 1000 records\n",
    "\n",
    "given_training_text = given_training_text.head(1000)\n",
    "given_testing_text = given_testing_text.head(1000)\n",
    "given_training_label = given_training_label.head(1000)\n",
    "given_testing_label = given_testing_label.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b35f1ef-fefc-4bfc-8992-3a10f2aa7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[feautures(i) for i in given_training_text['tweet']]\n",
    "test_data=[feautures(i) for i in given_testing_text['tweet']]\n",
    "x_training_data=np.array(train_data)\n",
    "x_testing_data=np.array(test_data)\n",
    "y_training_data = np.array(given_training_label['label'])\n",
    "y_testing_data = np.array(given_testing_label['label'])\n",
    "model=LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea33f89b-b2cc-4588-a12c-258a45eafafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 37.47636215917585\n",
      "Accuracy Score: 45.300000000000004\n"
     ]
    }
   ],
   "source": [
    "f1score =f1_score(y_testing_data, y_pred,average ='weighted')\n",
    "accscore=accuracy_score(y_testing_data, y_pred)\n",
    "print('F1 Score:', f1score*100)\n",
    "print('Accuracy Score:', accscore*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06e141-de00-478d-996e-85b861449dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
